{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8880552d-fa39-45aa-ad16-36d79a0302e7",
   "metadata": {},
   "source": [
    "## Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8de45-04de-46bc-a19b-c02191f0be04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports.\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "sns.set() #set Seaborn theme on plots.\n",
    "\n",
    "from datetime import date\n",
    "from transformers.models.bert import BertForMaskedLM\n",
    "from transformers.models.roberta import RobertaForMaskedLM, RobertaTokenizerFast\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, BertTokenizer\n",
    "from transformers import FillMaskPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a3c2f-250a-4b8f-99c9-cb371cedb3b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Constants & Directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335cf75-483a-4ed4-878f-f5b0b807fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameters.\n",
    "USE_GPU = True\n",
    "\n",
    "## Models.\n",
    "\n",
    "# BabyBERTa.\n",
    "# The authors made 3/10 random initializations of the model publicly available.\n",
    "MODELS = ['phueb/BabyBERTa-1', \n",
    "          'phueb/BabyBERTa-2', \n",
    "          'phueb/BabyBERTa-3', \n",
    "          'bert-base-cased', \n",
    "          'bert-large-cased', \n",
    "          'roberta-base',\n",
    "          'roberta-large',\n",
    "          \"nyu-mll/roberta-base-10M-1\",\n",
    "          \"nyu-mll/roberta-base-10M-2\",\n",
    "          \"nyu-mll/roberta-base-10M-3\",\n",
    "        ] \n",
    "SAVED_MODELS = ['saved_models/BabyBERTa_AO-CHILDES',\n",
    "                'saved_models/BabyBERTa_AO-CHILDES_standard_masking',\n",
    "                'saved_models/BabyBERTa_AO-CHILDES+AO-Newselsa+Wikipedia-1',\n",
    "                'saved_models/BabyBERTa_AO-Newsela',\n",
    "                'saved_models/BabyBERTa_Wikipedia-1',\n",
    "               ]\n",
    "\n",
    "## Data.\n",
    "\n",
    "# CHILDES.\n",
    "CHILDES = './data/CHILDES/aochildes.txt'\n",
    "\n",
    "# BLiMP.\n",
    "BLIMP = './data/BLiMP/' # data across many files.\n",
    "\n",
    "# Zorro.\n",
    "ZORRO = './data/Zorro/'\n",
    "ZORRO_SCRAMBLED = './data/Zorro_scrambled/'\n",
    "\n",
    "## Output Directories.\n",
    "BABYBERTA_CHILDES_OUT = './output/CHILDES/babyberta_childes_mlm.csv'\n",
    "BABYBERTA_BLIMP_OUT = './output/BLiMP/babyberta_blimp_mlm.csv'\n",
    "BABYBERTA_SCRAMBLED_ZORRO_OUT = './output/Zorro_scrambled/babyberta_zorro_scrambled_mlm.csv'\n",
    "\n",
    "ALL_MODELS_BLIMP_OUT = './output/BLiMP/all_models_blimp_mlm.csv'\n",
    "ALL_MODELS_ZORRO_OUT = './output/Zorro/all_models_zorro_mlm.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a4701b-9a02-48ec-8a77-192fe1ef30c9",
   "metadata": {},
   "source": [
    "## Torch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d545ff-f04c-4e5c-8abf-986c73a927da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up Torch to use the GPU if available.\n",
    "if USE_GPU and torch.cuda.is_available():  # Tell PyTorch to use the GPU. \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e57e1d-727c-47b0-94ae-b2478141ccb4",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56f6606-b731-4144-a706-bc643cf355f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to apply Masked Language Modeling to an entire sentence (pseudo log probability).\n",
    "def pseudo_log_prob(sentence, model, tokenizer, include_unigram=True, device=\"cpu\"):\n",
    "    '''\n",
    "    Helper function to apply Masked Language Modeling to an entire sentence (pseudo log probability).\n",
    "    This method applies the [MASK] token sequentially across each subword token of the sentence and\n",
    "    queries the model for the log-likelihood of the original token.  The log likelihoods are summed\n",
    "    across the entire sentence.  See Salazar et al. 2019 for comments.\n",
    "    '''    \n",
    "    logp_sentence = torch.tensor([0], dtype=torch.float64, device=device)\n",
    "    unigram_prob = []\n",
    "    # Get the full sentence embedding for MLM prediction.\n",
    "    sent_tokenized = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "    for i in range(1, len(sent_tokenized[\"input_ids\"][0])-1): # In order to skip the <start> and <end> tokens (vary by model).\n",
    "        # Grab the original token before masking.               \n",
    "        input_id = sent_tokenized[\"input_ids\"][0, i].clone() # Need to copy the token_id tensor.\n",
    "        #print(\"Original input id\", input_id)\n",
    "        \n",
    "        # Apply the <mask> token in order.\n",
    "        sent_tokenized[\"input_ids\"][0, i] = tokenizer.mask_token_id\n",
    "        #print(\"Input sentence:\", tokenizer.convert_ids_to_tokens(sent_tokenized[\"input_ids\"][0]))\n",
    "                        \n",
    "        # Get model outputs with softmax applied.  \n",
    "        # Note: outputs will have shape batch_size x sequence_length x vocab_size.\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**sent_tokenized, return_dict=True).logits.softmax(dim=2)\n",
    "        #print(outputs)\n",
    "        token_prob = outputs[0, i, input_id]#.detach().numpy()\n",
    "\n",
    "        # Remove the <mask> token from the input for the next iteration.\n",
    "        sent_tokenized[\"input_ids\"][0, i] = input_id\n",
    "        \n",
    "        # Aggregate.\n",
    "        logp_sentence += torch.log(token_prob)\n",
    "        \n",
    "        if not include_unigram: continue\n",
    "        token = tokenizer.convert_ids_to_tokens([input_id])[0]\n",
    "        unigram_prob.append({token:token_prob})\n",
    "        \n",
    "        #print(f\"Probability of token {token}: {token_prob}\")\n",
    "    \n",
    "    # This return will create new columns in the original dataframe.\n",
    "    #return pd.Series([logp_sentence, unigram_prob], index=[\"mlm_logprob\", \"unigram_mlm_logprob\"])\n",
    "    \n",
    "    # Unpack the pyTorch tensor.\n",
    "    logp_sentence = logp_sentence.to(\"cpu\").detach().numpy()[0]\n",
    "    output = {'sentence':sentence, \n",
    "            'pseudoLogProb':logp_sentence}\n",
    "    if include_unigram:\n",
    "        output['mlmUnigramProb'] = unigram_prob\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939d2a6-d6e0-48e7-bac4-aa737d8fc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to apply pseudo_log_prob to an entire DataFrame.\n",
    "def batch_pll(model_name, sentence_df, sent_col, include_unigram=False, device=\"cpu\"):\n",
    "    '''\n",
    "    Helper function to apply pseudo_log_prob to an entire DataFrame.  It works by vectorizing\n",
    "    pseudo_log_prob() and applying it to the pandas column of interest indicated by the\n",
    "    sent_col parameter.\n",
    "    '''\n",
    "    # Instantiate model with tokenizer.\n",
    "    if \"BabyBERTa\" in model_name:\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(model_name,\n",
    "                                          add_prefix_space=True,  # this must be added to produce intended behavior\n",
    "                                          )\n",
    "    elif \"roberta-base-10M-1\" in model_name:\n",
    "        model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    elif \"roberta\" in model_name:\n",
    "        model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "    elif \"bert\" in model_name:\n",
    "        model = BertForMaskedLM.from_pretrained(model_name)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    else:\n",
    "        print(\"No valid model found!  Terminating execution...\")\n",
    "        return\n",
    "    \n",
    "    # Send the model to the GPU if chosen.\n",
    "    model.to(device)\n",
    "    \n",
    "    ## Get MLM probability for each sentence in the sentences DataFrame.\n",
    "    results = np.vectorize(pseudo_log_prob)(sentence_df[sent_col], model, tokenizer, include_unigram=include_unigram, device=device)\n",
    "    results = pd.DataFrame(results.tolist())\n",
    "    \n",
    "    # Cleanup.\n",
    "    del model\n",
    "    del tokenizer    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b74245-5095-45a6-8c61-f4740d0148fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run all the models through a test set in minimal pair (BLiMP) format.\n",
    "def test_models(model_names, test_set, \n",
    "                good_sent_col=\"sentence_good\",\n",
    "                bad_sent_col=\"sentence_bad\", \n",
    "                output_dir=None,\n",
    "                device=\"cpu\"):\n",
    "    \n",
    "    if output_dir is not None and os.path.isfile(output_dir):\n",
    "        all_models = pd.read_csv(output_dir)\n",
    "    else:\n",
    "        all_models = pd.DataFrame(columns=(list(test_set.columns) + [\"model\", \"orig_min_pair_idx\", \"date\"]))\n",
    "\n",
    "    for model in model_names:\n",
    "        print(f\"Now testing {model}...\")\n",
    "        \n",
    "        model_good = batch_pll(model, test_set, good_sent_col, device=device)\n",
    "        model_bad = batch_pll(model, test_set, bad_sent_col, device=device)\n",
    "\n",
    "        # Error checking.\n",
    "        if model_good is None or model_bad is None:\n",
    "            print(f\"ERROR! Skipping... {model}\")\n",
    "\n",
    "        # Stitch together the two dataframes.\n",
    "        model_test = test_set.merge(model_good[\"pseudoLogProb\"], \n",
    "                                    left_index=True, \n",
    "                                    right_index=True, \n",
    "                                    copy=False, \n",
    "                                    validate=\"one_to_one\")\n",
    "        model_test = model_test.merge(model_bad[\"pseudoLogProb\"],\n",
    "                                      suffixes=(\"_good\", \"_bad\"),    # Should produce something like \"pseudoLogProb_good\", \"pseudoLogProb_bad\"...\n",
    "                                      left_index=True, \n",
    "                                      right_index=True, \n",
    "                                      copy=False, \n",
    "                                      validate=\"one_to_one\")\n",
    "\n",
    "        # Get model name.\n",
    "        sep = model.rfind(\"/\") # some model names may be paths.\n",
    "        model_name = model[sep + 1:] # if \"/\" is not a path, rfind() returns -1 so the index is just [0:]...\n",
    "        \n",
    "        # Tag which model produced this data.\n",
    "        model_test[\"model\"] = model_name\n",
    "\n",
    "        # Preserve the original index.\n",
    "        model_test.index.rename(\"orig_min_pair_idx\", inplace=True)\n",
    "        model_test.reset_index(inplace=True)\n",
    "\n",
    "        # Timestamp.\n",
    "        model_test[\"date\"] = date.today()\n",
    "\n",
    "        # Stack the dataframes.\n",
    "        all_models = pd.concat([all_models, model_test], ignore_index=True)\n",
    "\n",
    "        # Export in case something goes wrong...\n",
    "        if output_dir is not None:\n",
    "            all_models.to_csv(output_dir, index=False)\n",
    "    \n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f42ab7d-cbbc-4c9a-8798-0a993abdee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_by_phenomenon(output_df, model_col=\"model\", phenomenon_col=\"linguistics_term\", correct_col=\"correct\"):\n",
    "    # Create new dataframe.\n",
    "    results = pd.DataFrame(index=output_df[model_col].unique(), columns=output_df[phenomenon_col].unique())\n",
    "    \n",
    "    for idx, row in results.iterrows():\n",
    "        subset = output_df.loc[output_df[model_col] == idx]\n",
    "        for phenomenon in row.index:\n",
    "            row[phenomenon] = subset[correct_col].loc[subset[phenomenon_col] == phenomenon].mean()\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30870ce3-6ac6-40b3-a73a-57e499027d50",
   "metadata": {},
   "source": [
    "## Preprocessing & Data Loading.\n",
    "I want each of the datasets in its own DataFrame.  This may be especially tricky for the B|LiMP dataset because it's split across so many files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f26f4b-1e42-43bd-8dc8-d2014a6515c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLiMP.\n",
    "all_blimp_files = glob.glob(os.path.join(BLIMP, \"*.jsonl\"))\n",
    "blimp = pd.concat((pd.read_json(f, lines=True) for f in all_blimp_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00e03e-ef8b-435a-a038-9f2bc16ebb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd60b39-0630-40be-bb91-efe68ac3da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHILDES.\n",
    "with open(CHILDES) as f:\n",
    "    lines = [x.rstrip() for x in f.readlines()]\n",
    "    childes = pd.DataFrame({'sentence':lines})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcdcd1-c8fd-4380-9e36-b96914667ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zorro.\n",
    "all_zorro_files = glob.glob(os.path.join(ZORRO, \"*.txt\"))\n",
    "zorro = pd.DataFrame(columns=[\"sentence_good\", \"sentence_bad\", \"phenomenon\", \"paradigm\"])\n",
    "for f in all_zorro_files:\n",
    "    # Read file.\n",
    "    # File structure is a list of sentences arranged in minimal pairs.\n",
    "    frame = pd.read_csv(f, names=[\"sentence\"])\n",
    "    \n",
    "    # Split the DataFrame into good and bad sentences.\n",
    "    bad = frame.iloc[::2].reset_index(drop=True) # every other row, starting from the first one.\n",
    "    good = frame.iloc[1::2].reset_index(drop=True) # every other row, starting from the 2nd one.\n",
    "    \n",
    "    # Merge them to mirror BLiMP.\n",
    "    zorro_subset = good.merge(bad, \n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              suffixes=(\"_good\", \"_bad\"),\n",
    "                              validate=\"one_to_one\"\n",
    "                             )\n",
    "                     \n",
    "    # Clean up filename to annotate DataFrame.\n",
    "    start = f.rfind('/') + 1 # to strip path info.\n",
    "    end = -4                 # to eliminate .txt extension.\n",
    "    annotation = f[start:end]\n",
    "    \n",
    "    # Split filename into phenomenon and paradigm.\n",
    "    sep = annotation.rfind('-') # the last hyphen always separates phenomenon from paradigm.\n",
    "    phenomenon = annotation[:sep]\n",
    "    paradigm = annotation[sep+1:]\n",
    "    \n",
    "    # Annotate the DataFrame.\n",
    "    zorro_subset[\"phenomenon\"] = phenomenon\n",
    "    zorro_subset[\"paradigm\"] = paradigm\n",
    "    \n",
    "    # Add to full Zorro.\n",
    "    zorro = pd.concat([zorro, zorro_subset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6d434-2a26-4ea0-864a-6f4d534a1bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "zorro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60483b-3fec-4b4f-a6c3-15d66368c720",
   "metadata": {},
   "source": [
    "## Load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff4cf0-09d6-479d-af20-41c724b1803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(MODELS[0],\n",
    "                                                  add_prefix_space=True,  # this must be added to produce intended behavior\n",
    "                                          )\n",
    "model = RobertaForMaskedLM.from_pretrained(MODELS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a181f-ea33-452e-813f-f3c1b832d03b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Send the model to the GPU if available.\n",
    "model.to(device)  # This shouldn't do anything if no GPU was available (device=\"cpu\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79243b09-0eca-4d69-bc17-83d136ab6c84",
   "metadata": {},
   "source": [
    "## Single sentence tests\n",
    "<b>Note:</b> The RoBERTa tokenizer by design uses the _Ġ_ character to indicate a whitespace.  Notice only either whole-word tokens or word-initial subword pieces have the _Ġ_ marker prepended.  The default BERT model uses ## instead.  This is supposedly an engineering trick to improve performance.\n",
    "\n",
    "Sources:\n",
    "- https://stackoverflow.com/questions/61134275/difficulty-in-understanding-the-tokenizer-used-in-roberta-model\n",
    "- https://github.com/openai/gpt-2/issues/80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f37261-9e09-4348-ac5e-c7dd0531bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_log_prob(\"The even ones are the grammatical ones.\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11600d8-4875-42e8-8b67-1fd65b2149ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_log_prob(\"The even ones is the grammatical ones.\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9409d772-8de2-4e85-8470-c95825ac03b6",
   "metadata": {},
   "source": [
    "## Run inference on all the data.\n",
    "I will start by deleting the model and tokenizer from earlier.  RAM and VRAM are limited, and the helper functions should abstract the loading and unloading of all the components already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee50bd-ef8f-4e63-b3fa-3db6230d6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleanup from the earlier inference...\n",
    "del tokenizer\n",
    "del model\n",
    "\n",
    "# I want to get these off memory.  \n",
    "# The helper function should take care of loading and unloading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254fff8-aac1-485a-82a4-bb737452990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "babyberta_childes = batch_pll(MODELS[0], childes, \"sentence\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73451b57-b085-4909-86f0-bf6db5a7bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "babyberta_blimp_good = batch_pll(MODELS[0], blimp, \"sentence_good\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37536079-3cf7-4163-b96d-df14c5c9fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "babyberta_blimp_bad = batch_pll(MODELS[0], blimp, \"sentence_bad\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58f1f7-872c-428c-9383-3c995bcf1258",
   "metadata": {},
   "source": [
    "## Export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a4f91-82ad-4b94-bab9-63a27987f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the BLiMP good and bad sentences for exporting.\n",
    "new_cols = [\"pseudoLogProb\", \"mlmUnigramProb\"]\n",
    "babyberta_blimp = blimp.merge(babyberta_blimp_good[new_cols], \n",
    "                              left_index=True, \n",
    "                              right_index=True, \n",
    "                              copy=False, \n",
    "                              validate=\"one_to_one\")\n",
    "babyberta_blimp = babyberta_blimp.merge(babyberta_blimp_bad[new_cols],\n",
    "                                        suffixes=(\"_good\", \"_bad\"),    # Should produce something like \"pseudoLogProb_good\", \"pseudoLogProb_bad\"...\n",
    "                                        left_index=True, \n",
    "                                        right_index=True, \n",
    "                                        copy=False, \n",
    "                                        validate=\"one_to_one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1db60f-39cf-4e00-8f95-3b7fec33919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export.\n",
    "babyberta_childes.to_csv(BABYBERTA_CHILDES_OUT, index=False)\n",
    "babyberta_blimp.to_csv(BABYBERTA_BLIMP_OUT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbe1dc4-c4e4-4432-9a7f-60fb548f0704",
   "metadata": {},
   "source": [
    "## Preliminary Analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb17f93-52bf-4573-b0ee-3675b98b3f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How did BabyBERTa do on BLiMP?\n",
    "babyberta_blimp_correct = (babyberta_blimp.pseudoLogProb_good > babyberta_blimp.pseudoLogProb_bad).astype(int)\n",
    "babyberta_blimp_avg_perf = blimp_correct.sum()/blimp_correct.size\n",
    "print(f\"Overall {MODELS[0]} performance on BLiMP:{babyberta_blimp_correct.sum()}/{babyberta_blimp_correct.size} ({babyberta_blimp_avg_perf}) \\n\")\n",
    "\n",
    "# Plot.\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.barplot(babyberta_blimp, x=\"linguistics_term\", y=blimp_correct, errorbar=None, ax=ax)\n",
    "ax.bar_label(ax.containers[-1], fmt='Mean:\\n%.2f', label_type='edge') # Label the bars.\n",
    "# Include overall average performance.\n",
    "ax.axhline(y=babyberta_blimp_avg_perf, label='BabyBERTA-1 mean BLiMP perf.', linestyle='--');\n",
    "\n",
    "# Styling and labeling.\n",
    "ax.set_ylim([0.0, 1.0])\n",
    "plt.xticks(rotation=45, ha='right');\n",
    "ax.legend();\n",
    "ax.set_title(\"BabyBERTa per-phenomenon performance on BLiMP\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32ec51-b18a-49c7-b3b9-85fa9c631634",
   "metadata": {},
   "source": [
    "## Custom distractors.\n",
    "I will take sentences from the `distractor_agreement_relation_noun.json` and `distractor_agreement_relative_clause.json` files and modify them to make them not quite as easy as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61811b64-0ce5-4d96-b8a0-2fb0f61dac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## But first...\n",
    "sv_agreement_phenomena = [\"distractor_agreement_relation_noun\", \"distractor_agreement_relative_clause\"]\n",
    "# NOTE: I am querying for the FILES and not what the authors listed as a SUBJECT VERB AGREEMENT PHENOMENON.\n",
    "babyberta_sv_agreement = babyberta_blimp.loc[(babyberta_blimp.UID.isin(sv_agreement_phenomena))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7df829-4ba8-4816-b286-df58a2792657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let me filter down the data to make it manageable.\n",
    "relevant_sv_agree_columns = [\"UID\", \"sentence_good\", \"pseudoLogProb_good\", \"sentence_bad\", \"pseudoLogProb_bad\", \"linguistics_term\", \"lexically_identical\",]\n",
    "babyberta_sv_agreement = babyberta_sv_agreement[relevant_sv_agree_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f89a98-1a96-461d-87fe-1a108e6e6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a correctness column.\n",
    "babyberta_sv_agreement[\"correct\"] = (babyberta_sv_agreement.pseudoLogProb_good > babyberta_sv_agreement.pseudoLogProb_bad).astype(int)\n",
    "print(\"How well did BabyBERTa do on these off the bat?\")\n",
    "babyberta_sv_agreement.correct.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d29c2-7e50-4ed6-a4ea-c7bddf317d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I would like to see a few examples it got right:\")\n",
    "for idx, row in babyberta_sv_agreement.loc[babyberta_sv_agreement.correct == 1].sample(10).iterrows():\n",
    "    print(f\"|------------Row #{idx}--------------\")\n",
    "    print(f\"| {idx}a. {row.sentence_good} | good sentence PLL: {row.pseudoLogProb_good}\") \n",
    "    print(f\"| *{idx}b. {row.sentence_bad} | bad sentence PLL: {row.pseudoLogProb_bad}\") \n",
    "    print(\"############################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ba04d-66d7-4045-8448-043463318b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"... and some it got wrong:\")\n",
    "for idx, row in babyberta_sv_agreement.loc[babyberta_sv_agreement.correct == 0].sample(10).iterrows():\n",
    "    print(f\"|------------Row #{idx}--------------\")\n",
    "    print(f\"| {idx}a. {row.sentence_good} | good sentence PLL: {row.pseudoLogProb_good}\") \n",
    "    print(f\"| *{idx}b. {row.sentence_bad} | bad sentence PLL: {row.pseudoLogProb_bad}\") \n",
    "    print(\"############################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e05a2-873b-4d9b-a2ac-02cfc31f9711",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Large scale simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7567f1-30d4-44d3-969a-a66c8847c6d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_models_blimp = test_models(SAVED_MODELS + MODELS, blimp, output_dir=ALL_MODELS_BLIMP_OUT, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6942d-975c-4890-8047-86205897bc15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "babyberta_zorro = test_models(SAVED_MODELS + MODELS, zorro, output_dir=ALL_MODELS_ZORRO_OUT, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8581a-e5fb-40b7-bfb1-0d7783892120",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b85024-353c-4a32-a511-6e71979603dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_models = [\"phueb/BabyBERTa-1\", \"phueb/BabyBERTa-2\", \"phueb/BabyBERTa-3\", \"bert-base-cased\", \"bert-large-cased\", \"roberta-base\", \"roberta-large\"]\n",
    "ordered_blimp_phenomena = [\"anaphor_agreement\", \"argument_structure\", \"binding\", \"control_raising\", \"determiner_noun_agreement\", \"ellipsis\", \"filler_gap_dependency\", \"irregular_forms\", \"island_effects\", \"npi_licensing\", \"quantifiers\", \"subject_verb_agreement\"]\n",
    "ordered_zorro_phenomena = [\"anaphor_agreement\", \"argument_structure\", \"binding\", \"case\", \"determiner_noun_agreement\", \"ellipsis\", \"filler_gap_dependency\", \"irregular_forms\", \"island_effects\", \"local_attractor\", \"npi_licensing\", \"quantifiers\", \"subject_verb_agreement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd2ad6-1078-4590-940a-658db0f06114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## How did the BERT models do on BLiMP?\n",
    "all_models_blimp[\"correct\"] = (all_models_blimp.pseudoLogProb_good > all_models_blimp.pseudoLogProb_bad).astype(int)\n",
    "for model in all_models_blimp.model.unique():\n",
    "    subset = all_models_blimp.loc[all_models_blimp.model == model]\n",
    "    print(f\"Overall {model} performance on BLiMP:\" +\\\n",
    "          f\"{subset.correct.sum()}/{subset.size} ({np.mean(subset.correct)}) \\n\")\n",
    "\n",
    "# Plot.\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "sns.barplot(all_models_blimp, x=\"linguistics_term\", y=\"correct\", hue=\"model\", errorbar=None, ax=ax)\n",
    "#ax.bar_label(ax.containers[-1], fmt='Mean:\\n%.2f', label_type='edge') # Label the bars.\n",
    "# Include overall average performance.\n",
    "#ax.axhline(y=babyberta_blimp_avg_perf, label='BabyBERTA-1 mean BLiMP perf.', linestyle='--');\n",
    "\n",
    "# Styling and labeling.\n",
    "ax.set_ylim([0.0, 1.0])\n",
    "plt.xticks(rotation=45, ha='right');\n",
    "ax.legend();\n",
    "ax.set_title(\"BERT-derived models' per-phenomenon performance on BLiMP\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a509e92-fa0a-419c-b265-4b8f09d0e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How did the BERT models do on BLiMP?\n",
    "all_models_zorro[\"correct\"] = (all_models_zorro.pseudoLogProb_good > all_models_zorro.pseudoLogProb_bad).astype(int)\n",
    "for model in all_models_zorro.model.unique():\n",
    "    subset = all_models_zorro.loc[all_models_zorro.model == model]\n",
    "    print(f\"Overall {model} performance on Zorro:\" +\\\n",
    "          f\"{subset.correct.sum()}/{subset.size} ({np.mean(subset.correct)}) \\n\")\n",
    "\n",
    "# Plot.\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "sns.barplot(all_models_zorro, x=\"phenomenon\", y=\"correct\", hue=\"model\", errorbar=None, ax=ax)\n",
    "#ax.bar_label(ax.containers[-1], fmt='Mean:\\n%.2f', label_type='edge') # Label the bars.\n",
    "# Include overall average performance.\n",
    "#ax.axhline(y=babyberta_blimp_avg_perf, label='BabyBERTA-1 mean BLiMP perf.', linestyle='--');\n",
    "\n",
    "# Styling and labeling.\n",
    "ax.set_ylim([0.0, 1.0])\n",
    "plt.xticks(rotation=45, ha='right');\n",
    "ax.legend();\n",
    "ax.set_title(\"BERT-derived models' per-phenomenon performance on Zorro\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8ce77-e6d1-466e-9405-6f92735994fc",
   "metadata": {},
   "source": [
    "## Scrambling Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e71040-a3c8-4f98-82e1-3bb7509e147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to assemble Zorro.\n",
    "def assemble_zorro(zorro_directory):\n",
    "    all_zorro_files = glob.glob(os.path.join(zorro_directory, \"*.txt\"))\n",
    "    zorro = pd.DataFrame(columns=[\"sentence_good\", \"sentence_bad\", \"phenomenon\", \"paradigm\"])\n",
    "    for f in all_zorro_files:\n",
    "        # Read file.\n",
    "        # File structure is a list of sentences arranged in minimal pairs.\n",
    "        frame = pd.read_csv(f, names=[\"sentence\"])\n",
    "\n",
    "        # Split the DataFrame into good and bad sentences.\n",
    "        bad = frame.iloc[::2].reset_index(drop=True) # every other row, starting from the first one.\n",
    "        good = frame.iloc[1::2].reset_index(drop=True) # every other row, starting from the 2nd one.\n",
    "\n",
    "        # Merge them to mirror BLiMP.\n",
    "        zorro_subset = good.merge(bad, \n",
    "                                  left_index=True,\n",
    "                                  right_index=True,\n",
    "                                  suffixes=(\"_good\", \"_bad\"),\n",
    "                                  validate=\"one_to_one\"\n",
    "                                 )\n",
    "\n",
    "        # Clean up filename to annotate DataFrame.\n",
    "        start = f.rfind('/') + 1 # to strip path info.\n",
    "        end = -4                 # to eliminate .txt extension.\n",
    "        annotation = f[start:end]\n",
    "\n",
    "        # Split filename into phenomenon and paradigm.\n",
    "        sep = annotation.rfind('-') # the last hyphen always separates phenomenon from paradigm.\n",
    "        phenomenon = annotation[:sep]\n",
    "        paradigm = annotation[sep+1:]\n",
    "\n",
    "        # Annotate the DataFrame.\n",
    "        zorro_subset[\"phenomenon\"] = phenomenon\n",
    "        zorro_subset[\"paradigm\"] = paradigm\n",
    "\n",
    "        # Add to full Zorro.\n",
    "        zorro = pd.concat([zorro, zorro_subset], ignore_index=True)\n",
    "    return zorro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49458c48-5999-4b30-9814-7bbae857ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BabyBERTa models on scrambled Zorro data.\n",
    "models_to_test = ['saved_models/BabyBERTa_AO-CHILDES',\n",
    "                  'saved_models/BabyBERTa_AO-CHILDES_standard_masking',\n",
    "                 ]\n",
    "\n",
    "seed_count = 0 # For keeping track of the different scrambled Zorro runs.\n",
    "for seed in os.listdir(ZORRO_SCRAMBLED):\n",
    "    # Check that the path is not a hidden folder.\n",
    "    if seed.startswith('.'): continue\n",
    "\n",
    "    # load Scrambled Zorro.\n",
    "    zorro_dir = os.path.join(ZORRO_SCRAMBLED, seed)\n",
    "    zorro_scrambled = assemble_zorro(zorro_dir)\n",
    "\n",
    "    # Tag this particular scrambled dataset.\n",
    "    zorro_scrambled[\"scrambling_seed\"] = seed_count\n",
    "    seed_count +=1 # update the seed counter.\n",
    "    \n",
    "    # Run the simulation.\n",
    "    babyberta_zorro = test_models(models_to_test, \n",
    "                                  zorro_scrambled, \n",
    "                                  output_dir=BABYBERTA_SCRAMBLED_ZORRO_OUT, \n",
    "                                  device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add96dd-ec81-473b-8da5-257646ef8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(ZORRO_SCRAMBLED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fe92c-02e0-4f53-9ef9-c2776ec5f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.isdir(ZORRO_SCRAMBLED + \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc366e2e-0852-435a-8873-858c1f71c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.path.join(ZORRO_SCRAMBLED, \"0\"), \"*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b504c3-f830-4c1a-9a4b-fc1acfbaa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(zorro_directory, \"*.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
